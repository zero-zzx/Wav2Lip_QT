# Wav2Lip_QT
虚拟化身——2D虚拟人语音驱动算法


论文:https://zhuanlan.zhihu.com/p/520683296
Wav2Lip使用了一个基于深度学习的神经网络模型来实现音频与口型动作视频的同步。
具体来说，Wav2Lip使用了一种称为条件生成对抗网络（Conditional GAN）的模型。
该模型包含了两个主要的神经网络，即生成器网络和判别器网络。

生成器网络的作用是接受音频信号作为输入，然后生成对应的口型动作视频。
判别器网络的作用是接受真实的口型动作视f频和生成器生成的视频作为输入，然后判断哪一个是真实的。

训练时，Wav2Lip将音频和对应的口型动作视频作为输入，然后通过生成器网络生成合成的视频，
并将其与真实的口型动作视频一起输入判别器网络进行训练。
目标是让生成的视频尽可能地接近真实的视频，同时判别器网络需要能够准确地区分生成的视频和真实的视频。
经过训练后，生成器网络可以接受音频信号作为输入，并生成与之相对应的逼真的口型动作视频。

Wav2Lip 代码库中包含了四个预训练的模型文件，分别是：
2 个嘴唇运动生成器（lip sync generator）模型，用于将音频转换为嘴唇运动。分别为“wav2lip_gan.pth”和“wav2lip_disc.pth”。
1 个人脸检测器（face detector）模型，用于在视频或图像中检测出人脸，以便将嘴唇运动应用到正确的位置。模型文件为“face_detection.pth”。
1 个姿态估计器（pose estimator）模型，用于检测人脸的姿态（例如头部的旋转和倾斜角度），以便更好地调整嘴唇运动。模型文件为“pose_detection.pth”。
这些模型文件是预训练的，并且在使用 Wav2Lip 进行口型运动生成时需要使用它们。
